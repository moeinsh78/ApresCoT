from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from typing import List, Set
from tqdm import tqdm
import pandas as pd
import numpy as np
import os

# Resolve the absolute path to the project root
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
HF_MODELS_DIR = os.path.join(PROJECT_ROOT, 'hf_models')


NODE_SIMILARITY_THRESHOLD = 0.9
EDGE_SIMILARITY_THRESHOLD = 0.7
TRIPLES_MATCHING_SIMILARITY_THRESHOLD = 0.85

def match_nodes_using_embeddings(nodes_set: Set[str], llm_final_answers: List[str]):
    """
    Match nodes in the subgraph with the final answers generated by LLM using sentence embeddings
    """
    answer_match_status = []
    node_to_answer_id = {}
    nodes_list = list(nodes_set)
    encoded_nodes = get_sentence_encodings(nodes_list)

    for i, answer in enumerate(llm_final_answers):
        answer_embedding = get_sentence_encodings([answer])
        most_similar_node_id = get_most_similar_context_sentence_id(answer_embedding, encoded_nodes, answer, nodes_list, NODE_SIMILARITY_THRESHOLD)
        
        # print("Most Similar Node ID:", most_similar_node_id)
        if most_similar_node_id != -1:
            matched_node = nodes_list[most_similar_node_id - 1]
            # print("Most Similar Node:", matched_node, "\n\n")
            node_to_answer_id[matched_node] = i + 1
            answer_match_status.append({"Answer": answer, "Index": i + 1})
        else:
            # print("No Similar Node Found!\n\n")
            answer_match_status.append({"Answer": answer, "Index": "No Match"})

    return answer_match_status, node_to_answer_id

def match_nodes(nodes_set: Set[str], llm_final_answers: List[str]):
    answer_match_status = []
    node_to_answer_id = {}
    for i, answer in enumerate(llm_final_answers):
        if answer in nodes_set:
            node_to_answer_id[answer] = i + 1
            answer_match_status.append({"Answer": answer, "Index": i + 1})
        else:
            answer_match_status.append({"Answer": answer, "Index": "No Match"})

    return answer_match_status, node_to_answer_id


def match_edges(edge_descriptions: List[str], llm_cot: List[str], cot_in_triples: bool):
    threshold = EDGE_SIMILARITY_THRESHOLD
    if cot_in_triples:
        threshold = TRIPLES_MATCHING_SIMILARITY_THRESHOLD

    encoded_edge_descriptions = get_sentence_encodings(edge_descriptions)
    
    matched_cot_list = []
    context_to_cot_match = {}
    for i, cot_step in enumerate(llm_cot):
        # print("Reasoning step: ", cot_step)
        cot_step_embedding = get_sentence_encodings([cot_step])
        most_similar_context_sentence_id = get_most_similar_context_sentence_id(cot_step_embedding, encoded_edge_descriptions, None, None, threshold)
        
        # print("Most Similar Sentence ID:", most_similar_context_sentence_id)
        if most_similar_context_sentence_id != -1:
            # print("Most Similar Sentence:", edge_descriptions[most_similar_context_sentence_id - 1])
            # print("\n\n")
            matched_cot_list.append({"COT Step": cot_step, "Most Similar Context ID": most_similar_context_sentence_id})
            context_to_cot_match[most_similar_context_sentence_id] = i + 1
        else:
            # print("No Similar Sentence Found!\n\n")
            matched_cot_list.append({"COT Step": cot_step, "Most Similar Context ID": "No Match"})

    return matched_cot_list, context_to_cot_match



def get_sentence_encodings(sentence_list):
    """
    Get sentence embeddings using Sentence Transformer (MiniLM)
    """
    model = SentenceTransformer(
        'sentence-transformers/all-MiniLM-L6-v2',
        cache_folder=HF_MODELS_DIR
    )
    # model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    embeddings = model.encode(sentence_list)

    return embeddings



def get_most_similar_context_sentence_id(cot_step_embedding, context_embeddings, answer, nodes, threshold):
    """
    Get most similar context sentence ID for a given CoT step or answer embedding by 
    calculating cosine similarity between CoT step and context sentences
    """
    similarity_scores = cosine_similarity(context_embeddings, cot_step_embedding).flatten()
    # print("Similarities to: ", answer)
    # if answer is not None:
    #     for node, score in zip(nodes, similarity_scores):
    #         print(node, score)

    most_similar_context_sentence_id = np.argmax(similarity_scores) + 1
    if (similarity_scores[most_similar_context_sentence_id - 1] < threshold):
        return -1

    return most_similar_context_sentence_id



# def get_sentence_encodings(sentence_list):
#     tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
#     model = DistilBertModel.from_pretrained("distilbert-base-uncased")

#     context_sentences_df = pd.DataFrame(sentence_list, columns=['sentences'])
#     context_sentences_df["ids"] = context_sentences_df.index

#     batch_size = 1
#     embeddings = []

#     context_sentences_list = list(context_sentences_df.sentences)
#     encoded_context_sentences = tokenizer.batch_encode_plus(context_sentences_list, return_tensors='pt', max_length=512, padding=True, truncation=True)

#     for i in tqdm(range(0, len(encoded_context_sentences['input_ids']), batch_size)):
#         batch_input_ids = encoded_context_sentences['input_ids'][i:i + batch_size]
#         batch_attention_mask = encoded_context_sentences['attention_mask'][i:i + batch_size]
    
#         outputs = model(batch_input_ids, attention_mask=batch_attention_mask)
    
#         batch_embeddings = outputs.last_hidden_state[:,0,:]
#         batch_embeddings = batch_embeddings.detach().numpy()

#         embeddings.append(batch_embeddings)

#     embeddings = np.concatenate(embeddings, axis=0)
    
#     return embeddings



# def get_most_similar_context_sentence_id(cot_step: str, context_embeddings):
#     tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
#     model = DistilBertModel.from_pretrained("distilbert-base-uncased")

#     encoded_cot_step = tokenizer(cot_step, return_tensors='pt', max_length=512, padding=True, truncation=True)

#     cot_step_vector_embedding = model(**encoded_cot_step).last_hidden_state[:, 0, :].detach().numpy()

#     # print("Context Embeddings Shape: ", context_embeddings.shape)
#     # print("COT Step Embeddings Shape: ", cot_step_vector_embedding.shape)
#     similarity_scores = cosine_similarity(context_embeddings, cot_step_vector_embedding).flatten()

#     print("Chain of Thought Step: ", cot_step)
#     print("Similarity Scores:\n", similarity_scores)

#     most_similar_context_sentence_id = np.argmax(similarity_scores) + 1
#     return most_similar_context_sentence_id

